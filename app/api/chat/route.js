export const runtime = 'nodejs'; // Node.js í™˜ê²½ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)

import OpenAI from "openai";
import fs from "fs";
import path from "path";

// ---- OpenAI ì´ˆê¸°í™” ----
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// ---- Feldman ê¸°ë³¸ ì•ˆë‚´ í”„ë¡¬í”„íŠ¸ ----
const systemPrompt = `
You are a helpful museum docent guiding users through Edmund Feldman's 4-step art critique.
Steps:
1) Description (ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?)
2) Analysis (êµ¬ì„±/ëŒ€ë¹„/ê· í˜• ê´€ê³„ëŠ”?)
3) Interpretation (ì˜ë¯¸/ë§¥ë½/ìƒì§•ì€?)
4) Judgment (ê·¼ê±° ê¸°ë°˜ í‰ê°€)
í•œêµ­ì–´ë¡œ ì§§ê²Œ ë¬¼ì–´ë³´ë©° ëŒ€í™”í•˜ì„¸ìš”.
ì‚¬ìš©ìê°€ ê° ë‹¨ê³„ì— ì„±ì‹¤íˆ ë‹µí•˜ë©´, ë‹¤ìŒ ë‹¨ê³„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°€ì„¸ìš”.
`;

// ---- RAG ë°ì´í„° ê²½ë¡œ (embedding vectors) ----
const DATA_PATH = path.join(
  process.cwd(),
  "app/data/rag/embeddings/critiques/dccp_feldman_4step_vectors.json"
);

// ---- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ----
function cosineSim(a, b) {
  let dot = 0,
    na = 0,
    nb = 0;
  for (let i = 0; i < a.length; i++) {
    dot += a[i] * b[i];
    na += a[i] * a[i];
    nb += b[i] * b[i];
  }
  return dot / (Math.sqrt(na) * Math.sqrt(nb));
}

// ---- ì§ˆë¬¸ ê°ì§€ ----
function isQuestion(text) {
  const qTriggers = ["?", "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡", "ì•Œë ¤ì¤˜", "ë­ì•¼", "ì„¤ëª…"];
  return qTriggers.some((kw) => text.includes(kw));
}

// ---- RAG ë¬¸ì„œ ë¡œë“œ ----
let DOCS = null;
function loadDocs() {
  if (!DOCS) {
    DOCS = JSON.parse(fs.readFileSync(DATA_PATH, "utf-8"));
  }
  return DOCS;
}

// ---- RAG ê²€ìƒ‰ í•¨ìˆ˜ ----
async function retrieveContext(question) {
  const docs = loadDocs();

  // 1ï¸âƒ£ ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
  const embRes = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: question,
  });
  const qvec = embRes.data[0].embedding;

  // 2ï¸âƒ£ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 5ê°œ ì„ íƒ
  const top = docs
    .map((d) => ({
      ...d,
      score: cosineSim(qvec, d.embedding), // âœ… í•„ë“œëª… ë§ì¶¤ (d.vector â†’ d.embedding)
    }))
    .sort((a, b) => b.score - a.score)
    .slice(0, 5);

  // 3ï¸âƒ£ ì°¸ê³  ë¬¸ë§¥ êµ¬ì„±
  const context = top
    .map(
      (d, i) =>
        `ã€${i + 1}ã€‘(${d.stage.toUpperCase()}) ${d.text}`
    )
    .join("\n\n");

  return context;
}

// ----- ë©”ì¸ POST í•¸ë“¤ëŸ¬ -----
export async function POST(req) {
  try {
    const { messages = [] } = await req.json();

    // âœ… ë§ˆì§€ë§‰ ìœ ì € ë©”ì‹œì§€ content ì²˜ë¦¬ (ë¬¸ìì—´ or ë°°ì—´ ëª¨ë‘ ëŒ€ì‘)
    let lastUserMessage = "";
    const last = messages[messages.length - 1];
    if (Array.isArray(last?.content)) {
      lastUserMessage = last.content
        .map((c) => (typeof c.text === "string" ? c.text : ""))
        .join(" ")
        .trim();
    } else {
      lastUserMessage = last?.content || "";
    }

    // ---- í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™” ----
    let systemMessage = systemPrompt;
    let finalMessages = [];

    // ğŸ¯ ì§ˆë¬¸ì¸ ê²½ìš° â†’ RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if (isQuestion(lastUserMessage)) {
      const context = await retrieveContext(lastUserMessage);
      systemMessage = `
ë‹¹ì‹ ì€ Feldman ë¯¸ìˆ  ë¹„í‰ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ê´€ë ¨ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤. ì´ë¥¼ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
ê°€ëŠ¥í•˜ë©´ ë‹¨ê³„ ê´€ë ¨ ìš©ì–´(ê¸°ìˆ , ë¶„ì„, í•´ì„, íŒë‹¨)ë¥¼ ì–¸ê¸‰í•˜ê³ , ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.

ì§ˆë¬¸: ${lastUserMessage}

ì°¸ê³ ìë£Œ:
${context}
      `;
      finalMessages = [{ role: "system", content: systemMessage }];
    } else {
      // ---- ì¼ë°˜ ëŒ€í™” íë¦„ (ë‹¨ê³„ ì•ˆë‚´)
      finalMessages = [{ role: "system", content: systemPrompt }, ...messages];
    }

    // ---- GPT ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ----
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      stream: true,
      temperature: 0.6,
      messages: finalMessages,
    });

    const encoder = new TextEncoder();

    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of completion) {
            const delta = chunk?.choices?.[0]?.delta?.content || "";
            if (delta) controller.enqueue(encoder.encode(delta));
          }
        } catch (err) {
          controller.error(err);
        } finally {
          controller.close();
        }
      },
    });

    // ---- ìŠ¤íŠ¸ë¦¬ë° Response ë°˜í™˜ ----
    return new Response(stream, {
      headers: {
        "Content-Type": "text/plain; charset=utf-8",
        "Cache-Control": "no-cache",
      },
    });
  } catch (e) {
    console.error("âŒ RAG/Chat Error:", e);
    return new Response(`Error: ${e.message}`, { status: 500 });
  }
}
